
\begin{center}
\section{RECURSIVE BINARY PARTITIONING \label{algo}}
\end{center}

We focus on regression models describing the conditional distribution of a
response variable $\Y$ given the status of $m$ covariates by
means of tree-structured recursive partitioning. The response $\Y$ from some
sample space $\sY$ may be multivariate as well. 
The $m$-dimensional covariate vector $\X = (X_1, \dots, X_m)$ is taken 
from a sample space $\sX = \sX_1 \times \cdots \times \sX_m$.
Both response variable and covariates may be measured
at arbitrary scales.
We assume that the conditional distribution $D(\Y | \X)$ of the response 
$\Y$ given the covariates $\X$ depends on a function $f$ of the covariates
\begin{eqnarray*} 
D(\Y | \X) = D(\Y | X_1, \dots, X_m) = D(\Y | f( X_1, \dots,
X_m)),
\end{eqnarray*}
where we restrict ourselves to partition based regression relationships,
i.e., $r$ disjoint cells $B_1, \dots, B_r$ partitioning the covariate space $\sX
= \bigcup_{k = 1}^r B_k$.
A model of the regression relationship is to be fitted based on a learning 
sample $\LS$, i.e., a random sample of $n$ independent and
identically distributed observations, possibly with some covariates $X_{ji}$
missing,
\begin{eqnarray*}
\LS & = & \{ (\Y_i, X_{1i}, \dots, X_{mi}); i = 1, \dots, n \}.
\end{eqnarray*}
A generic algorithm for recursive binary partitioning for a given learning
sample $\LS$ can be formulated using non-negative integer valued case weights $\w
= (w_1, \dots, w_n)$. Each node of a tree is represented by a vector of
case weights having non-zero elements when the corresponding observations
are elements of the node and are zero otherwise. The following generic 
algorithm implements recursive binary partitioning:
\begin{enumerate}
\item For case weights $\w$ test the global null hypothesis of independence between
      any of the $m$ covariates and the response. Stop if this 
      hypothesis cannot be rejected. 
      Otherwise select the $j^*$th covariate $X_{j^*}$ with strongest 
      association to $\Y$.
\item Choose a set $A^* \subset \sX_{j^*}$ in order to split $\sX_{j^*}$ into
      two disjoint sets $A^*$ and $\sX_{j^*} \setminus A^*$. 
      The case weights $\w_\text{left}$ and $\w_\text{right}$ determine the
      two subgroups with $w_{\text{left},i} = w_i I(X_{j^*i} \in A^*)$ and 
      $w_{\text{right},i} = w_i I(X_{j^*i} \not\in A^*)$ for all $i = 1,
      \dots, n$ ($I(\cdot)$ denotes the indicator function).
\item Recursively repeat steps 1 and 2 with modified case weights 
      $\w_\text{left}$ and $\w_\text{right}$, respectively.
\end{enumerate}
As we sketched in the introduction, the separation of variable
selection and splitting procedure into steps 1 and 2 of the algorithm
is the key for the construction of interpretable tree
structures not suffering a systematic tendency towards covariates with many
possible splits or many missing values. In addition, a statistically
motivated and intuitive stopping criterion can be implemented: We stop 
when the global null hypothesis of independence between the
response and any of the $m$ covariates cannot be rejected at a pre-specified
nominal level~$\alpha$. The algorithm induces a partition $\{B_1, \dots, B_r\}$ of
the covariate space $\sX$, where each cell $B \in \{B_1, \dots, B_r\}$ 
is associated with a vector of case weights. 
%%$w_i(B) = w_i I\left((X_{1i}, \dots,
%%X_{mi}) \in B\right)$. 
%%Some functional of the distribution of $\Y$ given $\X = \x$ can now be modeled by 
%%the observed values of the response $\Y$ and the weights 
%%$\w(\x) = \w(B), \x \in B,$ associated with the cell $B$ 
%%the measurements of $\x$ are element of. 

\begin{center}
\section{RECURSIVE PARTITIONING BY CONDITIONAL INFERENCE \label{framework}}
\end{center}

In the main part of this section we focus on step 1 of the generic algorithm.
Unified tests for independence are constructed by means of the conditional
distribution of linear statistics in the permutation test framework
developed by \cite{StrasserWeber1999}. The determination of the best binary split
in one selected covariate and the handling of missing values
is performed based on standardized linear statistics within the same
framework as well. 

\paragraph{Variable Selection and Stopping Criteria.}
At step 1 of the generic algorithm given in Section~\ref{algo} we face an 
independence problem. We need to decide whether there is any information
about the response variable covered by any of the $m$  covariates. In each node
identified by case weights $\w$, the
global hypothesis of independence is formulated in terms of the $m$ partial hypotheses
$H_0^j: D(\Y | X_j) = D(\Y)$ with global null hypothesis $H_0 = \bigcap_{j = 1}^m
H_0^j$.
When we are not able to reject $H_0$ at a pre-specified 
level $\alpha$, we stop the recursion.
If the global hypothesis can be rejected, we measure the association
between $\Y$ and each of the covariates $X_j, j = 1, \dots, m$, by
test statistics or $P$-values indicating the deviation from the partial
hypotheses $H_0^j$.  

For notational convenience and without loss of generality we assume that the
case weights $w_i$ are either zero or one. The symmetric group of all
permutations of  the elements of $(1, \dots, n)$ with corresponding case
weights $w_i = 1$ is denoted by $S(\LS, \w)$. A more general notation is
given in Appendix A. We measure the association between $\Y$ and $X_j, j = 1, \dots, m$, 
by linear statistics of the form
\begin{eqnarray} \label{linstat}
\T_j(\LS, \w) = \vec \left( \sum_{i=1}^n w_i g_j(X_{ji})
h(\Y_i, (\Y_1, \dots, \Y_n))^\top \right) \in \R^{p_jq}
\end{eqnarray}
where $g_j: \sX_j \rightarrow \R^{p_j}$ is a non-random transformation of
the covariate $X_j$. The \textit{influence function} 
$h: \sY \times \sY^n \rightarrow
\R^q$ depends on the responses $(\Y_1, \dots, \Y_n)$ in a permutation
symmetric way. 
%%, i.e., $h(\Y_i, (\Y_1, \dots, \Y_n)) = h(\Y_i, (\Y_{\sigma(1)}, \dots,
%%\Y_{\sigma(n)}))$ for all permutations $\sigma \in S(\LS, \w)$. 
Section~\ref{examples} explains how to choose $g_j$ and $h$ in different 
practical settings. A $p_j \times q$ matrix is converted into a 
$p_jq$ column vector by column-wise combination using the `vec' operator. 

The distribution of $\T_j(\LS, \w)$ under $H_0^j$ depends on the joint
distribution of $\Y$ and $X_j$, which is unknown under almost all practical
circumstances. At least under the null hypothesis one can dispose of this
dependency by fixing the covariates and conditioning on all possible 
permutations of the responses. This principle leads to test procedures known
as \textit{permutation tests}. 
The conditional expectation $\mu_j \in \R^{p_jq}$ and covariance $\Sigma_j
\in \R^{p_jq \times p_jq}$ of $\T_j(\LS, \w)$ under $H_0$ given
all permutations $\sigma \in S(\LS, \w)$ of the responses are derived by
\cite{StrasserWeber1999}: 

\begin{eqnarray}
\mu_j & = & \E(\T_j(\LS, \w) | S(\LS, \w)) = 
\vec \left( \left( \sum_{i = 1}^n w_i g_j(X_{ji}) \right) \E(h | S(\LS,
\w))^\top
\right), \nonumber \\
\Sigma_j & = & \V(\T_j(\LS, \w) | S(\LS, \w)) \nonumber \\
& = & 
    \frac{\ws}{\ws - 1}  \V(h | S(\LS, \w)) \otimes
        \left(\sum_i w_i  g_j(X_{ji}) \otimes w_i  g_j(X_{ji})^\top \right) \label{expectcovar}
\\
& - & \frac{1}{\ws - 1}  \V(h | S(\LS, \w))  \otimes \left(
        \sum_i w_i g_j(X_{ji}) \right)
\otimes \left( \sum_i w_i g_j(X_{ji})\right)^\top 
\nonumber
\end{eqnarray}
where $\ws = \sum_{i = 1}^n w_i$ denotes the sum of the case weights,
$\otimes$ is the Kronecker product and the conditional expectation of the
influence function is 
\begin{eqnarray*}
\E(h | S(\LS, \w)) = \ws^{-1} \sum_i w_i h(\Y_i, (\Y_1, \dots, \Y_n)) \in
\R^q
\end{eqnarray*} 
with corresponding $q \times q$ covariance matrix
\begin{eqnarray*}
\V(h | S(\LS, \w)) = \ws^{-1} \sum_i w_i \left(h(\Y_i, (\Y_1, \dots, \Y_n)) - \E(h | S(\LS, \w))
\right) \\
\left(h(\Y_i, (\Y_1, \dots, \Y_n)) - \E(h | S(\LS, \w))\right)^\top.
\end{eqnarray*}
Having the conditional expectation and covariance at hand we are able to
standardize a linear statistic $\T \in \R^{pq}$ of the form
(\ref{linstat}) for some $p \in \{p_1, \dots, p_m\}$. 
Univariate test statistics~$c$ mapping an observed multivariate 
linear statistic $\t \in
\R^{pq}$ into the real line can be of arbitrary form.  An obvious choice is
the maximum of the absolute values of the standardized linear statistic
\begin{eqnarray*}
c_\text{max}(\t, \mu, \Sigma)  = \max_{k = 1, \dots, pq} \left| \frac{(\t -
\mu)_k}{\sqrt{(\Sigma)_{kk}}} \right|
\end{eqnarray*}
utilizing the conditional expectation $\mu$ and covariance matrix
$\Sigma$. The application of a quadratic form $c_\text{quad}(\t, \mu, \Sigma)  = 
(\t - \mu) \Sigma^+ (\t - \mu)^\top$ is one alternative, although
computationally more expensive because the Moore-Penrose 
inverse $\Sigma^+$ of $\Sigma$ is involved.
It is important to note that the test statistics $c(\t_j, \mu_j, \Sigma_j),
j = 1, \dots, m$, cannot be directly compared in an unbiased  
way unless all of the covariates are measured at the same scale, i.e.,
$p_1 = p_j, j = 2, \dots, m$. 
In order to allow for an unbiased variable selection we need to switch to
the $P$-value scale because $P$-values for the conditional distribution of test
statistics $c(\T_j(\LS, \w), \mu_j, \Sigma_j)$ 
can be directly compared among covariates
measured at different scales. In step 1 of the generic algorithm we 
select the covariate with minimum $P$-value, i.e., the covariate $X_{j^*}$
with $j^*  =  \argmin_{j = 1, \dots, m} P_j$, where
\begin{displaymath}
P_j  =  \Prob_{H_0^j}(c(\T_j(\LS, \w), \mu_j,
            \Sigma_j) \ge c(\t_j, \mu_j, \Sigma_j) | S(\LS, \w))
\end{displaymath}
denotes the $P$-value of the conditional test for $H_0^j$. 

So far, we have only addressed testing each partial hypothesis $H_0^j$, which
is sufficient for an unbiased variable selection. A global test 
for $H_0$ required in step 1 can be constructed via an aggregation of the 
transformations $g_j, j = 1, \dots, m$,
i.e., using a linear statistic of the form
\begin{eqnarray*}
\T(\LS, \w) = \vec \left( \sum_{i=1}^n w_i
\left(g_1(X_{1i})^\top, \dots,
g_m(X_{mi})^\top\right)^\top h(\Y_i, (\Y_1, \dots, \Y_n))^\top \right).
%%\in \R^{\sum_j p_jq}
\end{eqnarray*}
However, this approach is less attractive for learning samples with
missing values. Universally applicable approaches are multiple test 
procedures based on $P_1, \dots, P_m$. 
Simple Bonferroni-adjusted $P$-values or a min-$P$-value resampling approach are just examples and we refer
to the multiple testing literature \citep[e.g.,][]{WestfallYoung1993}
for more advanced methods. We reject $H_0$ when the minimum 
of the adjusted $P$-values is less than a pre-specified nominal level $\alpha$
and otherwise stop the algorithm. In this sense, $\alpha$
may be seen as a unique parameter determining the size of the resulting trees.

The conditional distribution and thus the $P$-value 
of the statistic $c(\t, \mu, \Sigma)$ can be
computed in several different ways \citep[see][for an overview]{Z-papers:Hothorn+Hornik+VanDeWiel:2006}.
For some special forms of the
linear statistic, the exact distribution of the test statistic is tractable;
conditional Monte-Carlo procedures can always be used to approximate the exact
distribution. \cite{StrasserWeber1999} proved (Theorem 2.3) that the
conditional distribution of linear statistics $\T$ with conditional
expectation $\mu$ and covariance $\Sigma$ tends to a multivariate normal
distribution with parameters $\mu$ and $\Sigma$ as $n, \ws \rightarrow
\infty$. Thus, the asymptotic conditional distribution of test statistics of the
form $c_\text{max}$ is normal and
can be computed directly in the univariate case ($p_jq = 1$)
or approximated by means of quasi-randomized Monte-Carlo 
procedures in the multivariate setting \citep{numerical-:1992}. Quadratic forms
$c_\text{quad}$ follow a asymptotic 
$\chi^2$ distribution with degrees of freedom 
given by the rank of $\Sigma$ \citep[Theorem 6.20, ][]{Rasch1995},
and therefore asymptotic $P$-values can be computed efficiently.

\paragraph{Splitting Criteria.}
Once we have selected a covariate in step 1 of the algorithm, the split itself can be
established by any splitting criterion, including those established by
\cite{classifica:1984} or \cite{Shih1999}. Instead of simple binary splits, 
multiway splits can be implemented as well, for example utilizing
the work of \cite{OBrien2004}. However, most splitting criteria are not
applicable to response variables measured at arbitrary scales and we
therefore utilize the permutation test framework described above  
to find the optimal binary split in one selected 
covariate $X_{j^*}$ in step~2 of the generic algorithm. The goodness of a split is
evaluated by two-sample linear statistics which are special cases of the linear 
statistic (\ref{linstat}). 
For all possible subsets $A$ of the sample space $\sX_{j^*}$ the linear
statistic
\begin{eqnarray*}
\T^A_{j^*}(\LS, \w) = \vec \left( \sum_{i=1}^n w_i I(X_{j^*i} \in A) 
                                  h(\Y_i, (\Y_1, \dots, \Y_n))^\top \right) \in \R^{q}
\end{eqnarray*}
induces a two-sample statistic measuring the discrepancy between the samples 
$\{ \Y_i | w_i > 0 \text{ and } X_{ji} \in A; i = 1, \dots, n\}$ 
and $\{ \Y_i | w_i > 0 \text{ and } X_{ji} \not\in A; i = 1, \dots, n\}$. 
The conditional expectation $\mu_{j^*}^A$ and covariance
$\Sigma_{j^*}^A$
can be computed by (\ref{expectcovar}).
The split $A^*$ with a test statistic maximized over all possible
subsets $A$ is established:
\begin{eqnarray} \label{split}
A^* = \argmax_A c(\t_{j^*}^A, \mu_{j^*}^A, \Sigma_{j^*}^A).
\end{eqnarray}
Note that we do not need to compute the distribution of 
$c(\t_{j^*}^A, \mu_{j^*}^A, \Sigma_{j^*}^A)$ in step~2. 
In order to prevent pathological splits one can restrict the number of
possible subsets that are evaluated, for example by introducing restrictions
on the sample size or the sum of the case weights in each of the two groups
of observations induced by a possible split. 

\paragraph{Missing Values and Surrogate Splits.}

If an observation $X_{ji}$ in covariate $X_j$ is missing, we set the
corresponding case weight $w_i$ to zero for the computation of $\T_j(\LS, \w)$
and, if we would like to split in $X_j$, in $\T_{j}^A(\LS, \w)$ as well.
Once a split $A^*$ in $X_j$ has been implemented, surrogate splits can be 
established by
searching for a split leading to roughly the same division of the
observations as the original split. One simply replaces the original
response variable by a binary variable $I(X_{ji} \in A^*)$ coding the
split and proceeds as described in the previous part. 

\paragraph{Choice of $\alpha$.} 

The parameter $\alpha$ can be interpreted in
two different ways: as pre-specified nominal level of the
underlying association tests or as a simple hyper parameter determining the 
tree size. In the first sense, $\alpha$ controls the probability of falsely 
rejecting $H_0$ in each node. The typical conventions for balancing the type
I and type II errors apply in this situation. 

Although the test procedures used for constructing the tree are
general independence tests, they will only have high power for very
specific directions of deviation from independence (depending on the
choice of $g$ and $h$) and lower power for any other direction
of departure. Hence, a strategy to assure that any type of dependence is 
detected could be to increase the significance level $\alpha$. 
To avoid that the tree grown with a very large $\alpha$ overfits the data, 
a final step could be added for pruning the tree in a variety of ways,
for example by eliminating all terminal nodes until the terminal splits 
are significant at level $\alpha^\prime$,
with $\alpha^\prime$ being much smaller than the initial $\alpha$. 
Note, that by 
doing so the interpretation of $\alpha$ as nominal significance level
of conditional test procedures is lost. Moreover, $\alpha$ can be seen
as a hyper parameter that is subject to optimization with respect
to some risk estimate, e.g., computed via cross-validation
or additional test samples.

For explanatory modelling, the view of $\alpha$ as a significance level
seems more intuitive and easier to explain to subject matter
scientists, whereas for predictive modelling the view of $\alpha$ as a
hyper parameter is also feasible. Throughout the paper we adopt the
first approach and also evaluate it in a predictive setting in
Section~\ref{ec}.

\paragraph{Computational Complexity.}

The computational complexity of the variable selection step
is of order $n$ (for fixed
$p_j, j = 1, \dots, m$ and $q$) since
computing each $\T_j$ with corresponding $\mu_j$ and $\Sigma_j$ can be
performed in linear time. The computations of the test statistics $c$ is
independent of the number of observations. 
Searching the optimal splits in continuous variables involves ranking
these and hence is of order $n\log n$.
However, for nominal covariates measured at $K$ levels, 
the evaluation of all $2^{K-1} - 1$ possible splits is not necessary 
for the variable selection.


\begin{center}
\section{EXAMPLES \label{examples}}
\end{center}

\paragraph{Univariate Continuous or Discrete Regression.}
For a univariate numeric response $\Y \in \R$, the most natural influence function
is the identity $h(\Y_i, (\Y_1, \dots, \Y_n)) = \Y_i$. 
In cases where some observations with extremely large or small values have been
observed, a
ranking of the observations may be appropriate:
$h(\Y_i, (\Y_1, \dots, \Y_n)) = \sum_{k=1}^n w_k I(\Y_k \le \Y_i)$ for $i = 1, \dots,
n$.
Numeric covariates can be handled by the identity transformation 
$g_{ji}(x) = x$ (ranks or non-linear transformations are possible, too). 
Nominal covariates at levels $1, \dots, K$ are
represented by $g_{ji}(k) = e_K(k)$, the unit vector of length $K$ with
$k$th element being equal to one. Due to this flexibility, special test 
procedures like the Spearman test, the 
Wilcoxon-Mann-Whitney test or the Kruskal-Wallis test and permutation tests
based on ANOVA statistics or correlation coefficients
are covered by this framework. Splits obtained from (\ref{split}) maximize the
absolute value of the standardized difference between two means of the
values of the influence functions. 
For prediction, one is usually interested in an estimate of 
the expectation of
the response $\E(\Y | \X = \x)$ in each cell; an estimate 
can be obtained by 
\begin{eqnarray*}
\hat{\E}(\Y | \X = \x) = \left(\sum_{i=1}^n w_i(\x)\right)^{-1} \sum_{i=1}^n
w_i(\x) \Y_i,
\end{eqnarray*}
where $w_i(\x) = w_i$ when $\x$ is element of the same terminal node as the $i$th observation
and zero otherwise.

\paragraph{Censored Regression.}
The influence function $h$ may be chosen as 
logrank or Savage scores taking censoring into
account and one can proceed as for univariate continuous regression. This is
essentially the approach first published by \cite{regression:1988}. 
An alternative is the weighting scheme suggested by
\cite{MolinaroDudiotvdLaan2003}. A weighted Kaplan-Meier curve for the case
weights $\w(\x)$ can serve as prediction. 

\paragraph{$J$-Class Classification.}
The nominal response variable at levels $1, \dots, J$ 
is handled by influence
functions $h(\Y_i, (\Y_1, \dots, \Y_n)) = e_J(\Y_i)$. Note that for a
nominal covariate $X_j$ at levels $1, \dots, K$ with  
$g_{ji}(k) = e_K(k)$ the
corresponding linear statistic $\T_j$ is a vectorized contingency table 
of $X_j$ and $\Y$.
The conditional class probabilities can be estimated via 
\begin{eqnarray*}
\hat{\Prob}(\Y = y | \X = \x) = \left(\sum_{i=1}^n w_i(\x)\right)^{-1}
\sum_{i=1}^n
w_i(\x) I(\Y_i = y), \quad y = 1, \dots, J.
\end{eqnarray*}

\paragraph{Ordinal Regression.}
Ordinal response variables measured at $J$ levels, and ordinal covariates
measured at $K$ levels, are associated with score vectors $\xi \in
\R^J$ and $\gamma \in \R^K$, respectively. Those scores reflect the
`distances' between the levels: If the variable is derived from an
underlying continuous variable, the scores can be chosen as the midpoints
of the intervals defining the levels. The linear statistic is now a linear
combination of the linear statistic $\T_j$ of the form
\begin{eqnarray*}
\M \T_j(\LS, \w) & = & \vec \left( \sum_{i=1}^n w_i \gamma^\top g_j(X_{ji})
            \left(\xi^\top h(\Y_i, (\Y_1, \dots, \Y_n)\right)^\top \right)
\end{eqnarray*}
with $g_j(x) = e_K(x)$ and $h(\Y_i, (\Y_1, \dots, \Y_n)) = e_J(\Y_i)$.
If both response and covariate are ordinal, the matrix of coefficients
is given by the Kronecker product of both score vectors $\M = \xi \otimes \gamma \in
\R^{1, KJ}$. In case the response is ordinal only, the matrix of 
coefficients $\M$ is a block matrix 
\begin{eqnarray*}
\M = 
\left( 
    \begin{array}{ccc}
        \xi_1 &          & 0     \\
              &  \ddots  &       \\
          0   &          & \xi_1 
    \end{array} \right| 
%%\left.
%%    \begin{array}{ccc}
%%       \xi_2 &          & 0     \\
%%              &  \ddots  &       \\
%%          0   &          & \xi_2 
%%    \end{array} \right| 
%%\left.
    \begin{array}{c}
         \\
      \hdots \\
          \\
    \end{array} %%\right.
\left|
    \begin{array}{ccc}
        \xi_q &          & 0     \\
              &  \ddots  &       \\
          0   &          & \xi_q 
    \end{array} 
\right) 
%%\in \R^{K, KJ}
%%\end{eqnarray*}
\text{ or } %%and if one covariate is ordered 
%%\begin{eqnarray*}
%%\M = \left( 
%%    \begin{array}{cccc}
%%        \gamma & 0      &        & 0 \\
%%        0      & \gamma &        & \vdots \\
%%       0      & 0      & \hdots & 0 \\
%%        \vdots & \vdots &        & 0 \\
%%        0      & 0      &        & \gamma
%%    \end{array} 
%%    \right) 
\M = \text{diag}(\gamma)
%%\in \R^{J, KJ}
\end{eqnarray*}
when one covariate is ordered but the response is not. For both $\Y$ and $X_j$
being ordinal, the corresponding test is known as linear-by-linear association
test \citep{Agresti2002}. 
%%To our knowledge, there exist only two approaches
%%for recursive binary partitioning for ordinal regression. \cite{classifica:1984}
%%introduce the `ordered twoing' criterion (p.~108) and some implementations
%%of `CHAID' are able to deal with ordinal response variables. 

\paragraph{Multivariate Regression.}
For multivariate responses, the influence function is a combination of
influence functions appropriate for any of the univariate response variables
discussed in the previous paragraphs, e.g., indicators for multiple binary
responses \citep{Zhang1998,NohSongPark2004}, logrank or Savage scores
for multiple failure times 
%%\citep[for example tooth loss times, ][]{SuFan2004}
and the original observations or a rank transformation for multivariate regression 
\citep{Death2002}.
